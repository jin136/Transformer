{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import Sequential,Model,initializers,layers,Input\n",
    "\n",
    "class Data_augmentation(layers.Layer):\n",
    "    def __init__(self,num_classes,image_size):\n",
    "        super(Data_augmentation,self).__init__()\n",
    "        self.num_classes=num_classes\n",
    "        self.image_size=image_size\n",
    "\n",
    "        self.Normalization=layers.Normalization()\n",
    "        self.Resizing=layers.Resizing(self.image_size, self.image_size)\n",
    "        self.Randomflip=layers.RandomFlip(\"horizontal\")\n",
    "        self.Randomrotation=layers.RandomRotation(factor=0.02)\n",
    "        self.Randomzoom=layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        )\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x=self.Normalization(inputs)\n",
    "        x=self.Resizing(x)\n",
    "        x=self.Randomflip(x)\n",
    "        x=self.Randomrotation(x)\n",
    "        x=self.Randomzoom(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches,self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self,images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = tf.shape(patches)[-1]\n",
    "        patches = tf.reshape(patches, [batch_size,patches.shape[1]*patches.shape[2],patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim=projection_dim\n",
    "        self.projection = layers.Dense(units=self.projection_dim)\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            \"position_embeddings\", shape=[self.num_patches + 1, self.projection_dim],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "\n",
    "        self.classification_token = self.add_weight(\n",
    "            \"classification_token\", shape=[1, 1, self.projection_dim],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, patch):\n",
    "        #patch=self.projection(patch) # why using dense layer not 1d convolution?\n",
    "        cls_pos = tf.broadcast_to(\n",
    "            self.classification_token, [tf.shape(patch)[0], 1, self.projection_dim]) # (1,1,64) ->(batch,1,64)\n",
    "    \n",
    "        x = tf.concat([cls_pos, self.projection(patch)], axis=1) # (batch,1,64) (batch,64,64)->(batch,65,64)\n",
    "\n",
    "        x = x + self.positional_embedding  # (batch,65,64) + (65,64) -> (batch,65,64)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 65, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.zeros(shape=(8,65,64))\n",
    "b=tf.zeros(shape=(65,64))\n",
    "c=a+b\n",
    "c.shape\n",
    "#c=tf.concat([tf.zeros(shape=(8,1,64)),tf.zeros(shape=(8,1,64))],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(layers.Layer):\n",
    "    \n",
    "    def __init__(self,d_model,num_heads,dropout=0):\n",
    "        super(Multi_Head_Attention,self).__init__()\n",
    "        self.d_model=d_model #512\n",
    "        self.num_heads=num_heads #8\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.d_k=self.d_model//num_heads\n",
    "        self.dropout=dropout\n",
    "        self.wq = layers.Dense(self.d_model)\n",
    "        self.wk = layers.Dense(self.d_model)\n",
    "        self.wv = layers.Dense(self.d_model)\n",
    "        self.dense = layers.Dense(self.d_model)\n",
    "\n",
    "    def Split_Heads(self,input):\n",
    "        batch_size=tf.shape(input)[0]\n",
    "        seq_length=tf.shape(input)[1]\n",
    "        x=tf.reshape(input, [batch_size,seq_length, self.num_heads, self.d_k]) #(batch,65,8,8)    (batch,seq_length,num_head,d_k)\n",
    "        output=tf.transpose(x, perm=[0,2,1,3]) #(batch,8,65,8)      (batch,num_head,seq_length,d_k)\n",
    "        return output\n",
    "\n",
    "    def Scaled_Dot_Product_Attnetion(self,query,key,value):\n",
    "\n",
    "        key_dim=tf.cast(tf.shape(key)[-1],tf.float32) # 8\n",
    "        \n",
    "        query = tf.multiply(query, 1.0 / tf.sqrt(key_dim)) # Normalize (batch,8,65,8)\n",
    "       \n",
    "        attention_score=tf.matmul(query,key,transpose_a=False,transpose_b=True) #(batch,8,65,8) matmul (batch,8,8,65)-> (batch,8,65,65) (batch,num_head,seq_length,seq_length)\n",
    "   \n",
    "        attention_prob=tf.nn.softmax(attention_score,axis=-1) #(batch,8,65,65) \n",
    "\n",
    "        attention_prob=tf.nn.dropout(attention_prob,self.dropout)\n",
    "\n",
    "        attention_value=tf.matmul(attention_prob,value) #(batch,8,65,65) matmul (batch,8,65,8)-> (batch,8,65,8) (batch,num_head,seq_length,d_k)\n",
    "\n",
    "        return attention_value # 원문에서는 드랍아웃을 추가해서 Dropout 된거(attention output) 안된거(attention score) 둘다 리턴\n",
    "\n",
    "\n",
    "    def call(self,input):\n",
    "\n",
    "        batch_size=tf.shape(input)[0] # batch\n",
    "        seq_length=tf.shape(input)[1] # 65\n",
    "\n",
    "        query=input\n",
    "        key=input\n",
    "        value=input\n",
    "\n",
    "        q = self.wq(query) # (batch,65,64)\n",
    "        k = self.wk(key) # (batch,65,64)\n",
    "        v= self.wv(value) # (batch,65,64)\n",
    "\n",
    "        query=self.Split_Heads(q) #(batch,8,65,8) (batch,num_head,seq_length,d_k)\n",
    "        key=self.Split_Heads(k) #(batch,8,65,8)\n",
    "        value=self.Split_Heads(v) #(batch,65,8,8)\n",
    "    \n",
    "        concat_attention=self.Scaled_Dot_Product_Attnetion(query,key,value) # (batch,8,65,8) (batch,num_head,seq_length,seq_length)\n",
    "\n",
    "        concat_attention=tf.transpose(concat_attention,perm=[0,2,1,3]) # (batch,65,8,8) (batch,seq_length,num_head,key d_k)\n",
    "\n",
    "        concat_attention=tf.reshape(concat_attention,shape=(batch_size,seq_length,self.d_model)) # (batch,65,64) (batch,seq_length,d_model)\n",
    "\n",
    "        output=self.dense(concat_attention) # (batch,65,64) (batch,seq_length,d_model)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class MLP_ViT(layers.Layer):\n",
    "    def __init__(self,units,dropout_rate):\n",
    "        self.units=units\n",
    "        self.dropout_rate=dropout_rate\n",
    "        self.dropout=layers.Dropout(self.dropout_rate)\n",
    "        self.mlp1=layers.Dense(units=2048,activation=tf.nn.gelu)\n",
    "        self.mlp2=layers.Dense(units=self.units,activation=tf.nn.gelu)\n",
    "        super(MLP_ViT,self).__init__()\n",
    "\n",
    "    def call(self,input):\n",
    "        x=self.mlp1(input) # (batch,65,2048) \n",
    "        x=self.dropout(x)\n",
    "        x=self.mlp2(x) # (batch,65,64) \n",
    "        x=self.dropout(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,Input\n",
    "\n",
    "class ViT():\n",
    "    def __init__(self,input_size,num_classes,patch_size,projection_dim,transformer_layers,d_model,num_heads,dropout_MHA,dropout_MLP):\n",
    "        self.input_size= input_size\n",
    "        self.num_classes=num_classes\n",
    "        self.image_size=self.input_size[0]\n",
    "        self.patch_size=patch_size\n",
    "        self.num_patches=(self.image_size//self.patch_size)**2\n",
    "        self.projection_dim=projection_dim\n",
    "        self.transformer_layers=transformer_layers\n",
    "        self.MLP_units=[self.projection_dim*2,self.projection_dim]\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.dropout_MHA=dropout_MHA\n",
    "        self.dropout_MLP=dropout_MLP\n",
    "        self.output=layers.Dense(units=self.num_classes)\n",
    "        \n",
    "    def Encoder_Block(self,input):\n",
    "        x=layers.LayerNormalization(epsilon=1e-6)(input)\n",
    "        x=Multi_Head_Attention(num_heads=self.num_heads, d_model=self.d_model)(x)\n",
    "        x1=layers.Add()([x,input])\n",
    "        x2=layers.LayerNormalization(epsilon=1e-6)(x1)\n",
    "        x2=MLP_ViT(units=self.d_model,dropout_rate=self.dropout_MLP)(x2)\n",
    "        x3=layers.Add()([x1,x2])\n",
    "        return x3\n",
    "\n",
    "    def call(self):\n",
    "        input=Input(shape=(self.input_size))\n",
    "        x=Data_augmentation(num_classes=self.num_classes,image_size=self.image_size)(input) # (batch,6)\n",
    "        x=Patches(patch_size=self.patch_size)(x)\n",
    "        x=PatchEncoder(num_patches=self.num_patches,projection_dim=self.projection_dim)(x)\n",
    "        for i in range(self.transformer_layers):\n",
    "            x=self.Encoder_Block(x)\n",
    "\n",
    "        x=MLP_ViT(units=2048,dropout_rate=self.dropout_MLP)(x) # (batch,65,2048) (batch,seq_length,2048)\n",
    "        print(x.shape)\n",
    "        x=layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x=layers.GlobalAveragePooling1D()(x)\n",
    "        #x=layers.Flatten()(x)\n",
    "        output=self.output(x)\n",
    "        model=Model(inputs=input,outputs=output)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 785, 2048)\n"
     ]
    }
   ],
   "source": [
    "model=ViT( \n",
    "    input_size=(224,224,3),\n",
    "    num_classes=100,\n",
    "    patch_size=8,\n",
    "    projection_dim=256,\n",
    "    transformer_layers=8,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    dropout_MHA=0.5,\n",
    "    dropout_MLP=0).call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model,to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " data_augmentation_20 (Data_aug  (None, 224, 224, 3)  7          ['input_23[0][0]']               \n",
      " mentation)                                                                                       \n",
      "                                                                                                  \n",
      " patches_20 (Patches)           (None, 784, 192)     0           ['data_augmentation_20[0][0]']   \n",
      "                                                                                                  \n",
      " patch_encoder_20 (PatchEncoder  (None, 785, 256)    250624      ['patches_20[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " layer_normalization_324 (Layer  (None, 785, 256)    512         ['patch_encoder_20[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_113 (Mu  (None, 785, 256)    263168      ['layer_normalization_324[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_304 (Add)                  (None, 785, 256)     0           ['multi__head__attention_113[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'patch_encoder_20[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_325 (Layer  (None, 785, 256)    512         ['add_304[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_166 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_325[0][0]']\n",
      "                                                                                                  \n",
      " add_305 (Add)                  (None, 785, 256)     0           ['add_304[0][0]',                \n",
      "                                                                  'mlp__vi_t_166[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_326 (Layer  (None, 785, 256)    512         ['add_305[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_114 (Mu  (None, 785, 256)    263168      ['layer_normalization_326[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_306 (Add)                  (None, 785, 256)     0           ['multi__head__attention_114[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_305[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_327 (Layer  (None, 785, 256)    512         ['add_306[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_167 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_327[0][0]']\n",
      "                                                                                                  \n",
      " add_307 (Add)                  (None, 785, 256)     0           ['add_306[0][0]',                \n",
      "                                                                  'mlp__vi_t_167[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_328 (Layer  (None, 785, 256)    512         ['add_307[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_115 (Mu  (None, 785, 256)    263168      ['layer_normalization_328[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_308 (Add)                  (None, 785, 256)     0           ['multi__head__attention_115[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_307[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_329 (Layer  (None, 785, 256)    512         ['add_308[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_168 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_329[0][0]']\n",
      "                                                                                                  \n",
      " add_309 (Add)                  (None, 785, 256)     0           ['add_308[0][0]',                \n",
      "                                                                  'mlp__vi_t_168[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_330 (Layer  (None, 785, 256)    512         ['add_309[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_116 (Mu  (None, 785, 256)    263168      ['layer_normalization_330[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_310 (Add)                  (None, 785, 256)     0           ['multi__head__attention_116[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_309[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_331 (Layer  (None, 785, 256)    512         ['add_310[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_169 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_331[0][0]']\n",
      "                                                                                                  \n",
      " add_311 (Add)                  (None, 785, 256)     0           ['add_310[0][0]',                \n",
      "                                                                  'mlp__vi_t_169[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_332 (Layer  (None, 785, 256)    512         ['add_311[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_117 (Mu  (None, 785, 256)    263168      ['layer_normalization_332[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_312 (Add)                  (None, 785, 256)     0           ['multi__head__attention_117[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_311[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_333 (Layer  (None, 785, 256)    512         ['add_312[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_170 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_333[0][0]']\n",
      "                                                                                                  \n",
      " add_313 (Add)                  (None, 785, 256)     0           ['add_312[0][0]',                \n",
      "                                                                  'mlp__vi_t_170[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_334 (Layer  (None, 785, 256)    512         ['add_313[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_118 (Mu  (None, 785, 256)    263168      ['layer_normalization_334[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_314 (Add)                  (None, 785, 256)     0           ['multi__head__attention_118[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_313[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_335 (Layer  (None, 785, 256)    512         ['add_314[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_171 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_335[0][0]']\n",
      "                                                                                                  \n",
      " add_315 (Add)                  (None, 785, 256)     0           ['add_314[0][0]',                \n",
      "                                                                  'mlp__vi_t_171[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_336 (Layer  (None, 785, 256)    512         ['add_315[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_119 (Mu  (None, 785, 256)    263168      ['layer_normalization_336[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_316 (Add)                  (None, 785, 256)     0           ['multi__head__attention_119[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_315[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_337 (Layer  (None, 785, 256)    512         ['add_316[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_172 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_337[0][0]']\n",
      "                                                                                                  \n",
      " add_317 (Add)                  (None, 785, 256)     0           ['add_316[0][0]',                \n",
      "                                                                  'mlp__vi_t_172[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_338 (Layer  (None, 785, 256)    512         ['add_317[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi__head__attention_120 (Mu  (None, 785, 256)    263168      ['layer_normalization_338[0][0]']\n",
      " lti_Head_Attention)                                                                              \n",
      "                                                                                                  \n",
      " add_318 (Add)                  (None, 785, 256)     0           ['multi__head__attention_120[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'add_317[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_339 (Layer  (None, 785, 256)    512         ['add_318[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " mlp__vi_t_173 (MLP_ViT)        (None, 785, 256)     1050880     ['layer_normalization_339[0][0]']\n",
      "                                                                                                  \n",
      " add_319 (Add)                  (None, 785, 256)     0           ['add_318[0][0]',                \n",
      "                                                                  'mlp__vi_t_173[0][0]']          \n",
      "                                                                                                  \n",
      " mlp__vi_t_174 (MLP_ViT)        (None, 785, 2048)    4722688     ['add_319[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_340 (Layer  (None, 785, 2048)   4096        ['mlp__vi_t_174[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_16 (G  (None, 2048)        0           ['layer_normalization_340[0][0]']\n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dense_824 (Dense)              (None, 100)          204900      ['global_average_pooling1d_16[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,702,891\n",
      "Trainable params: 15,702,884\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate=tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 10000, 0.97, staircase=False, name=None)\n",
    "\n",
    "    \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_rate),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12639 files belonging to 100 classes.\n",
      "Using 11376 files for training.\n",
      "Found 12639 files belonging to 100 classes.\n",
      "Using 1263 files for validation.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      " 610/1422 [===========>..................] - ETA: 4:28 - loss: 5.1984 - acc: 0.0090"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m lr_rate\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mschedules\u001b[39m.\u001b[39mExponentialDecay(\u001b[39m1e-3\u001b[39m, \u001b[39m10000\u001b[39m, \u001b[39m0.97\u001b[39m, staircase\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlr_rate),loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 48\u001b[0m history\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mfit(train_ds,validation_data\u001b[39m=\u001b[39;49mvalidation_ds,epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49mbatch_size,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir=r'D:\\dataset\\butterfly\\train'\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "train_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "labels=\"inferred\",\n",
    "label_mode=\"int\",\n",
    "class_names=None,\n",
    "color_mode=\"rgb\",\n",
    "batch_size=batch_size,\n",
    "image_size=(224, 224),\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "validation_split=0.1,\n",
    "subset='training',\n",
    "interpolation=\"gaussian\",\n",
    "follow_links=False,\n",
    "crop_to_aspect_ratio=False,)\n",
    "\n",
    "\n",
    "validation_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "labels=\"inferred\",\n",
    "label_mode=\"int\",\n",
    "class_names=None,\n",
    "color_mode=\"rgb\",\n",
    "batch_size=batch_size,\n",
    "image_size=(224, 224),\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "validation_split=0.1,\n",
    "subset='validation',\n",
    "interpolation=\"gaussian\",\n",
    "follow_links=False,\n",
    "crop_to_aspect_ratio=False,)\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_ds=validation_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "\n",
    "lr_rate=tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 10000, 0.97, staircase=False, name=None)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_rate),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['acc'])\n",
    "\n",
    "history=model.fit(train_ds,validation_data=validation_ds,epochs=100,batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_19\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 32, 32, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mcifar100\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m      7\u001b[0m batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m\n\u001b[1;32m----> 9\u001b[0m history\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mx_train,y\u001b[39m=\u001b[39;49my_train,validation_data\u001b[39m=\u001b[39;49m(x_test,y_test),epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49mbatch_size,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileknxliuhb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jin\\anaconda3\\envs\\tf_210_py_390\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_19\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "lr_rate=tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 10000, 0.97, staircase=False, name=None)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_rate),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['acc'])\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "history=model.fit(x=x_train,y=y_train,validation_data=(x_test,y_test),epochs=100,batch_size=batch_size,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_210_py_390",
   "language": "python",
   "name": "tf_210_py_390"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e354fddcd8e05f362b5f6e797eff9ab9958198386d58cfc3a129f5c90614cc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
