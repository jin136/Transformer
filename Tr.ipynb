{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Input,Model,Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\jin\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:08<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:10<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00,  7.67s/ url]\n",
      "Extraction completed...: 100%|██████████| 112/112 [00:11<00:00, 10.16 file/s]\n",
      "Dl Size...: 100%|██████████| 124/124 [00:11<00:00, 11.25 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:11<00:00, 11.03s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to C:\\Users\\jin\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Text' has no attribute 'SubwordTextEncoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m BUFFER_SIZE \u001b[39m=\u001b[39m \u001b[39m20000\u001b[39m\n\u001b[0;32m      2\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tokenizer_en \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mText\u001b[39m.\u001b[39;49mSubwordTextEncoder\u001b[39m.\u001b[39mbuild_from_corpus(\n\u001b[0;32m      4\u001b[0m     (en\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m pt, en \u001b[39min\u001b[39;00m train_examples), target_vocab_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m13\u001b[39m)\n\u001b[0;32m      6\u001b[0m tokenizer_pt \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mText\u001b[39m.\u001b[39mSubwordTextEncoder\u001b[39m.\u001b[39mbuild_from_corpus(\n\u001b[0;32m      7\u001b[0m     (pt\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m pt, en \u001b[39min\u001b[39;00m train_examples), target_vocab_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m13\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(lang1, lang2):\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Text' has no attribute 'SubwordTextEncoder'"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "tokenizer_en = tfds.features.Text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.Text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return lang1, lang2\n",
    "\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_pt, result_en\n",
    "\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# 데이터셋을 메모리에 캐싱하여 데이터를 읽는 속도를 높입니다.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, ((None,),(None,)))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE, ((None,),(None,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_string \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTransformer is awesome.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tokenized_string \u001b[39m=\u001b[39m tokenizer_en\u001b[39m.\u001b[39mencode(sample_string)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m토큰화된 문자열은 \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m 입니다\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(tokenized_string))\n\u001b[0;32m      6\u001b[0m original_string \u001b[39m=\u001b[39m tokenizer_en\u001b[39m.\u001b[39mdecode(tokenized_string)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_en' is not defined"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('토큰화된 문자열은 {} 입니다'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('원래 문자열: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # 패딩을 넣기 위해 어텐션 로짓(logit)에 추가적인 차원을 넣습니다.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에 사인(sin)을 적용합니다: 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 배열의 홀수 인덱스에 코사인(cos)을 적용합니다: 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # matmul_qk를 스케일링합니다\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # 스케일링된 텐서에 마스크를 더합니다.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # 소프트 맥스의 마지막 축(seq_len_k)을 정규화하여 스코어의 합이 1이 되도록 만듭니다.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 임베딩과 포지션 인코딩을 추가합니다.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                    look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m dff \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[0;32m      4\u001b[0m num_heads \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[1;32m----> 6\u001b[0m input_vocab_size \u001b[39m=\u001b[39m tokenizer_pt\u001b[39m.\u001b[39mvocab_size \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      7\u001b[0m target_vocab_size \u001b[39m=\u001b[39m tokenizer_en\u001b[39m.\u001b[39mvocab_size \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      8\u001b[0m dropout_rate \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_pt' is not defined"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_210_py_390",
   "language": "python",
   "name": "tf_210_py_390"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25197282c967dd160e08ad66fbaa0f5ea8754d42e8970faa7c009ddd9acfff81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
